%1 : Why ABM?


Travel demand models have during the last decades evolved from highly aggregated trip based models, through tour based models into activity based models that considers the choice of full daily activity-travel patterns including all activities and all transportation at an individual or household level. Activity based models sets out to model the interdependence of all activity-travel episodes performed during the day as trips are naturally linked in space-time. As demand for travel is derived from the demand for activity participation, demand for activities should be a determinant for if, when and where trips are being performed. Modelling the interdependence of all aspects of an activity-travel pattern together with time-space constraints is especially important when considering policy changes such as congestion charge that are becoming increasingly important in today's traffic planning, as how people react to such policies depend on how they trade-off between, e.g., arrival time to work, cost and mode of transport. 

To achieve an interdependent choice a full daily travel pattern, it is common to assume that individuals (or households) make the decision of how to plan their day based on some sort of scoring or utility function including all activity and travel episodes of the day. Possible the first model based on this assumption was presented by \citet{Adler79}, who assumed that individuals behaved as if they choose the utility maximising travel pattern among the set of all feasible travel patterns.
To actually implement a model based on this assumption in 1979 they had to impose some additional restrictions: firstly the choice set was limited to observed travel patterns; and secondly timing of trips were not modelled. One of the great challenges with implementing a model for the choice of a full activity-travel pattern is still the immense size of the choice set, especially when timing of trips are considered. Alternatives methods that does not require choice set formation has therefore been developed. In the Household Activity Pattern Problem (HAPP), the choice of all household members daily travel patterns including time-of-day, mode and location choices is formulated as constrained mixed-integer optimisation problem \citep{recker2001bridge,Recker08,Recker13,yuan2014HAPP}. The problem is solvable, but computationally very demanding and with only 19 locations the computation time is reported to an average of $614\units{s/household}$ \citep{Recker13}. The choice of an activity-travel pattern can also be formulated as a shortest-path problem in a an abstract network, as in the multistate-supernetwork model \citep{arentze04Multistate}. In such a network, states can be defined for remaining mandatory activites and thus capture time-space constraints \citep{liao2013incorporating,liao2016modeling}. Similarly to HAPP, the computation time is still a limiting factor. \citet{liao2016modeling} reports that the evaluating the choice of a daily travel pattern takes $8\units{s}$ when an individual has two fixed activities to perform in a day and 6 locations to choose from.
%It is further not yet clear how to estimate the model, as parameters currently are obtained by using separate MNL models for different choice dimensions in \citet{Liao2017}. The model estimated is thus not identical with the shortest path problem solved when the model is used for simulation in, e.g., \citet{liao2016modeling}.

Some models, e.g.,  Starchild \citep{recker86starchild1,recker86starchild2}, have avoided the need to formulate the universal choice set by constructing a random process by which alternative travel patterns are derived, and compare a smaller set of alternative travel patterns obtained from this process based on their full utilities.  
A similar approach is also used in MATSim which besides mode, timing and location also include route choice for of all trips although the number of activities and their ordering is fixed  \citep{lefebvre2007fast,grether2009mode,balmer05,horni2012high}.  AMOS \citep{kitamura1996Sams} set out to mimic how individuals adapt their travel patterns in the event of changes to environment and evaluate whether the perturbations offer any improvement in terms of full daily utility. Somewhat similarly, AURORA focuses on how individuals reschedule a given set of activities by adjusting their activity-travel pattern until further adjustments does not motivate the search cost \citep{timmermans2001modeling,joh2003Aurora, johEstimationAurora2005}.

An alternative to consider the choice of a full daily travel pattern as a joint choice has been to model a sequential choice of, e.g., whether or not to perform a trip, departure time, mode of transport, destination and purpose. The probability to make a new choice is then modelled conditional on previous choices, and a sequence of choices will produce a daily travel pattern. This is the approach taken in, e.g., Albatross \citep{timmermans2001modeling} and CEMDAP \citep{bhat2004comprehensive}. In such sequential models, the full history can influence the probability in any choice situation, ensuring an interdependence between trips. \citet{Habib11RUM} developed a discrete-continuous random utility model for weekend travelling where agents decided on mode, destination and activity based on the utility of the combination. Agents in the model of \citet{Habib11RUM} also considered how their decisions influenced future opportunities by including a utility component for the value of future time. The value of future time was modelled as a time-of-day dependent composite good, which was parameterized and estimated. 

A sequential model where a part of the history influences the probability of choices at the current point in time can be interpreted as a Markov process. Some work on activity planning has been explicitly formulating Markov chains to analyse the sequential dependence of different activity purposes. \citet{AllahviranlooMDP13} models how the probability of a specific activity by estimating transition probabilities from one activity to another based on socio-demographics and the history of previously performed activities, formulated as a Markov chain. \citet{susilo2014repetitions} used a Markov chain to model how activities performed on one day influences activities performed on subsequent days. \citet{hasan18} used a Markov chain to model sequential choices of activities and location in order to infer missing information in geo-location data gathered from social media.  


%Since all aspects of a travel pattern is interconnected, it seems reasonable to assume that individuals evaluate the full pattern when determining what activities to engage in as well as timing, location and mode of transport. Assuming that individuals (or households) act as if the consider the utility of a full daily travel pattern is not a new idea. It was proposed in \citet{Adler79}, who base a model on the assumption that households make a joint choice of a travel pattern $tp$, considering travel time, mode and destination choices for all tours during a day by selecting the optimal pattern $tp^*$ from the set of feasible patterns $TP$ according to: $tp^* = \argmax_{tp \in TP} U(tp) + \epsilon_{tp}$. With $\epsilon_{tp}$ Gumble distributed, this becomes an MNL model which was estimated using the observed sample as a choice set. Individuals are also considering the full daily travel pattern in: \citet{Recker08}, who formulates the model as a mathematical programming problem; and in MATSim \citep{balmer05,horni2011} who uses simulation to obtain close too optimal solutions. In AURORA \citep{joh2003Aurora,johEstimationAurora2005}, individuals are considering the utility of the full daily travel pattern but are assumed to use search-heuristics to schedule (and reschedule) their day. 
%%2 : ABM so far 

% For a comprehensive overview of activity based models, see e.g., \citet{Pinjari11} or \citet{Rasouli14}. 


The standard practice in the field of travel demand modelling is based on random utility maximisation, and even more specifically nested logit. One of the most extensive such models to date was initially presented in \citet{bowman2001} where a nested logit structure is developed for the choice of all trips performed during a day. The model treats tours and activities sequentially based on their importance for the individual. The model consists of five nests: 1) the choice of activity pattern, including the number of tours carried out during the day; 2) the choice of time of day for the primary tour and all its trips; 3) the mode and destination for the primary tour; 4) the time of day for the secondary tours; and 5) the mode and destination of the secondary tours. The nested logit structure ensures that higher decisions, such as the choice of activity pattern, includes the individual specific information about all available tour-combinations that the pattern includes. Many extensions to the original prototype model presented in \citet{bowman2001} has since been performed, to, e.g., allow for a greater detail in time-of-day choice \citep{Vovsha04}. In a recent implementation, time-of-day is modelled at a 30 minute resolution, where each individual consider a sampled subset of possible 30-minute intervals for each trip \citep{bradley2010sacsim}. In a discrete choice framework, it is logical to use the expected (maximum) utility from a day-path as input to higher order models and for cost appraisal \citep{Geurs10}. The expected utility could also be used to get detailed disaggregated measures of accessibility \citep{dong2006moving}.


% The probability of choosing a tour that ends at 5 PM should depend on the expected utility of the rest of a day when being home at 5 PM. If one more tour is to be conducted during the remaining time of the day, then they can be started at 10 alternative times before 10 PM. For the secondary tour, taking place after 5 PM, the expected utility should in turn depend on the starting time, since that will influence the number of available destinations that can be reached, as well as the time that can be spent on different activities. It should also depend on when one is finished with all activities.

%and \citep{bhat04} 

Although a large number of travel demand models  have been developed in the past, no model has to date managed to model the interdependent choice of a full day with a detailed time dimension while remaining consistent with utility maximisation and while being reasonably computationally demanding. Individual rationality is to-date a corner stone for welfare economics, enabling the approach to be theoretically and consistently translated into a tool for cost-benefit analysis. We see this argument as difficult to ignore without having a theory for behavioural welfare economics in sight. Further, non of the models discussed accounts for travel time uncertainty. Even when their purpose is to model rescheduling as in \citet{joh2003Aurora}, they assume that people are unaware of the possible need to reschedule when they make their decisions. Travel time uncertainty has been observed to have an important impact on departure time decisions and has been extensively studied using scheduling models following \citet{small82scheduling}. On the other hand, scheduling models up-to-date typically considers a single trip in isolation \citep[as in, e.g.,][]{fosgerau10reliability}, or possibly a chain of two trips \citep{jenelius11tripchain}. They may therefore not be suitable to analyse the system wide effects of more reliable travel times. Incorporating planning under uncertainty in a travel demand model is therefore an important topic. 

This paper builds on the proposed dynamic discrete choice model presented in \citet{karlstrom04} which \citet{jonsson2014reconciling} illustrated could be used to obtain give detailed time-of-day dependent accessibility measures, as demonstrated by \citet{jonsson2014reconciling}. In this paper, we show how a Markov Decision Process can be formulated to model the choice of a daily activity-travel pattern. 

In Section 2 of this paper, we show how a Markov Decision Process (MDP) can be used to model the choice of a daily activity-travel pattern. Following the random utility maximising approach, we assume that individuals maximise the expected achieved utility from a path through activity-location-time-space throughout one day. In the presence of stochasticity, a utility-maximizing agent will derive a decision policy outlining how to act conditional on the outcome of the stochastic process. Such a policy could be interpreted as contingency plan, specifying which travel pattern to choose for each possible combination of outcomes of the stochastic process. This is in contrast to available models where utility-maximisation of a activity-travel pattern has been modelled, e.g., Starchild, MATSim, HAPP and the multistate-supernetwork. One can formulate the choice of an activity-travel pattern as a possible policy where the decisions are fixed before the day start. However, a policy and especially the optimal policy could in general not be defined as a travel pattern, as it does not necessarily contain a fixed set of decisions independently of what happens. 

%In an MDP these transition probabilities are dependent on the choices of a decision maker and the interest lies in inferring the control law guiding their decisions. This is in in contrast to the Markov chain models discussed above where the interest lies in obtaining transition probabilities conditional on a specific state.

In Section 3, we show how the MDP developed in Section 2 can be formulated as a Dynamic discrete choice model (DDCM) following the work by \citet{rust1987} and thereby give state dependent choice probabilities that are a function not only of the state but also on the expected future utility conditional on the chosen action. We discuss how the expected future utility, i.e., the value function can be approximated using linear interpolation when time is modelled as a continuous variable and when travel time is uncertain. Using the obtained value functions it is possible to calculate choice probabilities and thus potentially use maximum likelihood estimation. Following \citet{fosgerau2013}, we show how the model in a special case degenerates into an MNL model over sequences of actions, and thus can be estimated using sampling of alternatives. Estimates obtained using sampling of alternatives are in contrast to direct maximum likelihood estimates not sensitive to approximations used when calculating the value function.
While similar to the approach developed in \citet{Habib11RUM}, this expected future utility is not parameterized but instead obtained as the expectation of  future one-stage utilities ans is therefore sensitive to changes in the transportation system which only influence the future and so, e.g., the choice of departure time to work in the morning will therefore be sensitive to the traffic conditions in the afternoon. The DDCM approach taken in this paper closely follows the standard nested logit practice, but introduces time explicitly, respecting that time has a direction and that decisions in real-life actually can be made sequentially in time, given the information available at each point in time when a decision is made. 

Observe that by using a DDCM formulation, the optimal decision is obtained using backward induction where every possible state-action combination is traversed, a process that can be computationally very demanding when the number of such combinations is large. It may not seem behaviourally realistic that individuals actually solve this complicated problem. As noted by \citet{RustML88}, an adapation and learning algorithm may provide a better explanation of how people may find the optimal decision rule in an MDP and thus act \emph{as if} they were utility maximisers. It is possible to find an approximate solution to an MDP using reinforcement learning which it has been argued represent a good description of how people plan their days \citep[see, e.g.,][]{arentze2004learning}. MDP's of activity-scheduling has also been solved using reinforcement learning by \citet{vanhusel09} to model the sequential planning of activities. \citet{Feygin18} proposed in his thesis how the decision rules in MDP for activity scheduling can be estimated using inverse reinforcement learning. \citet{karlstromScalingUp2009} also investigated how a boltzmann machine can be used to solve an MDP of the type proposed in this paper. 

In section 4, we present a case study were a model over a working day is specified and estimated on a travel survey. The case study presented in this paper excludes travel time uncertainty, and focus on the special case that can be estimated using sampling of alternatives. By focusing on this special case, we are able to assess the effects of the value function approximation on aggregate predictions. Section 5 discuss the results and section 6 concludes and discuss future work.  


