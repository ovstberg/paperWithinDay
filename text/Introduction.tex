%1 : Why ABM?


Travel demand models have during the last decades evolved from highly aggregated trip based models, through tour based models into activity based models that considers the choice of full daily activity-travel patterns including all activities and all transportation at an individual or household level. Activity based models sets out to model the interdependence of all activity-travel episodes performed during the day as trips are naturally linked in space-time. Further, as demand for travel is derived from the demand for activity participation, demand for activities should be a determinant for if, when and where trips are being performed. Modelling the interdependent choice an activity-travel pattern subject to time-space constraints may be especially important when forecasting the effects policy changes such as congestion charge that are becoming increasingly important in today's traffic planning, as how people react to such policies depend on how they trade-off between, e.g., arrival time to work, cost and mode of transport. 

To achieve a model of the interdependent choice a full daily travel pattern, it is common to assume that individuals (or households) preferences for such a pattern can be described by some scoring or utility function including all activity and travel episodes of the day. An early example is \citet{Adler79}, who assumed that individuals behaved as if they choose the utility maximising travel pattern among the set of all feasible travel patterns.
To actually implement a model based on this assumption in 1979 they had to impose some additional restrictions: firstly the choice set was limited to observed travel patterns; and secondly timing of trips were not modelled. One of the great challenges with implementing a model for the choice of a full activity-travel pattern is still the immense size of the choice set, especially when timing of trips are considered. Alternatives methods that does not require choice set formation has therefore been developed. In the Household Activity Pattern Problem (HAPP), the choice of all household members daily travel patterns including time-of-day, mode and location choices is formulated as constrained mixed-integer optimisation problem \citep{recker2001bridge,Recker08,Recker13,yuan2014HAPP}. The problem is solvable, but computationally very demanding and with only 19 locations the computation time is reported to an average of $614\units{s/household}$ \citep{Recker13}. The choice of an activity-travel pattern can also be formulated as a shortest-path problem in a an abstract network, as in the multistate-supernetwork model \citep{arentze04Multistate}. In such a network, states can be defined for remaining mandatory activites and thus capture time-space constraints \citep{liao2013incorporating,liao2016modeling}. Similarly to HAPP, the computation time is still a limiting factor. \citet{liao2016modeling} reports that the evaluating the choice of a daily travel pattern takes $8\units{s}$ when an individual has two fixed activities to perform in a day and 6 locations to choose from.
%It is further not yet clear how to estimate the model, as parameters currently are obtained by using separate MNL models for different choice dimensions in \citet{Liao2017}. The model estimated is thus not identical with the shortest path problem solved when the model is used for simulation in, e.g., \citet{liao2016modeling}.

Some models, e.g.,  Starchild \citep{recker86starchild1,recker86starchild2}, have avoided the need to formulate the universal choice set by constructing a random process by which alternative travel patterns are derived, and compare a smaller set of alternative travel patterns obtained from this process based on their full utilities.  
A similar approach is also used in MATSim which besides mode, timing and location also include route choice for of all trips although the number of activities and their ordering is fixed  \citep{lefebvre2007fast,grether2009mode,balmer05,horni2012high}.  AMOS \citep{kitamura1996Sams} set out to mimic how individuals adapt their travel patterns in the event of changes to environment and evaluate whether the perturbations offer any improvement in terms of full daily utility. Somewhat similarly, AURORA focuses on how individuals reschedule a given set of activities by adjusting their activity-travel pattern until further adjustments does not motivate the search cost \citep{timmermans2001modeling,joh2003Aurora, johEstimationAurora2005}.

The standard practice in the field of travel demand modelling is based on random utility maximisation, and even more specifically nested logit. One of the most extensive such models to date was initially presented in \citet{bowman2001} where a nested logit structure is developed for the choice of all trips performed during a day. The model treats tours and activities sequentially based on their importance for the individual. The model consists of five nests: 1) the choice of activity pattern, including the number of tours carried out during the day; 2) the choice of time of day for the primary tour and all its trips; 3) the mode and destination for the primary tour; 4) the time of day for the secondary tours; and 5) the mode and destination of the secondary tours. The nested logit structure ensures that higher decisions, such as the choice of activity pattern, includes the individual specific information about all available tour-combinations that the pattern includes. Many extensions to the original prototype model presented in \citet{bowman2001} has since been performed, to, e.g., allow for a greater detail in time-of-day choice \citep{Vovsha04}. In a recent implementation, time-of-day is modelled at a 30 minute resolution, where each individual consider a sampled subset of possible 30-minute intervals for each trip \citep{bradley2010sacsim}. In a discrete choice framework, it is logical to use the expected (maximum) utility from a day-path as input to higher order models and for cost appraisal \citep{Geurs10}. The expected utility could also be used to get detailed disaggregated measures of accessibility \citep{dong2006moving}.

An alternative to consider the choice of a full daily travel pattern as a joint choice has been to model a sequential choice of, e.g., whether or not to perform a trip, departure time, mode of transport, destination and purpose. The probability to make a new choice is then modelled conditional on previous choices, and a sequence of choices will produce a daily travel pattern. This is the approach taken in, e.g., Albatross \citep{timmermans2001modeling} and CEMDAP \citep{bhat2004comprehensive}. In such sequential models, the full history can influence the probability in any choice situation, ensuring an interdependence between trips. \citet{Habib11RUM} developed a discrete-continuous random utility model for weekend travelling where agents decided on mode, destination and activity based on the utility of the combination. Agents in the model of \citet{Habib11RUM} also considered how their decisions influenced future opportunities by including a utility component for the value of future time. The value of future time was modelled as a time-of-day dependent composite good, which was parameterized and estimated. 

A sequential model where a part of the history influences the probability of choices at the current point in time can be interpreted as a Markov process. Some work on activity planning has been explicitly formulating Markov chains to analyse the sequential dependence of different activity purposes. \citet{AllahviranlooMDP13} models how the probability of a specific activity by estimating transition probabilities from one activity to another based on socio-demographics and the history of previously performed activities, formulated as a Markov chain. \citet{susilo2014repetitions} used a Markov chain to model how activities performed on one day influences activities performed on subsequent days. \citet{hasan18} used a Markov chain to model sequential choices of activities and location in order to infer missing information in geo-location data gathered from social media.  

In this paper, we model the choice of a daily activity-travel pattern as a Markov Decision Process (MDP) which is further formulated as a Dynamic Discrete Choice Model (DDCM) to allow for estimation and simulating, extending the work presented in \citet{karlstrom04}. The DDCM approach taken in this paper closely follows the standard nested logit practice, but introduces time explicitly, respecting that time has a direction and that decisions in real-life actually can be made sequentially in time, given the information available at each point in time when a decision is made. Compared to the existing models of the nested-logit type, this allows for a greater detail and consistency in the time dimension. Especially, the DDCM approach allows for an explicit introduction of travel time uncertainty allowing such a model to be used to forecast the system wide effects of more reliable travel times. Travel time uncertainty has been observed to have an important impact on departure time decisions and has been extensively studied using scheduling models following \citet{small82scheduling}. On the other hand, scheduling models up-to-date typically considers a single trip in isolation \citep[as in, e.g.,][]{fosgerau10reliability}, or possibly a chain of two trips \citep{jenelius11tripchain}.
Similarly to nested logit based models, the DDCM model presented in this paper can theoretically and consistently
be translated into a tool for cost-benefit analysis. It can also be used to obtain time-of-day dependent accessibility measures taking into account the presence of mandatory activities, time-space constraints and travel time uncertainty, as illustrated by \citet{jonsson2014reconciling}.

This paper differ from and extend upon the work in \citet{karlstrom04} and \citet{jonsson2014reconciling} in a number of ways. We focus the choice of a daily activity-travel pattern, rather than a model of between day planning, and present a specification of an MDP which allows for incorporation of, e.g., time-space constraints in mandatory activities and dependence on previous mode and activities. We show how time can be modelled as a continuous variable using interpolation between grid-points in time-space, thus consistently allowing the model to incorporate travel time uncertainty. We also present a case study which shows that a model of the proposed type can indeed be estimated and used for simulation even with a large number of locations (1240). Previous specifications and implementations has only has only allowed for the evaluation of small example problems (with $\approx 20$ locations) where the evaluation of choice probabilities has still taken minutes even for single individuals.
 
 Finding the optimal decision rule in a DDCM requires evaluating every possible state-action combination, a process that can be computationally very demanding when the number of such combinations is large. It may not seem behaviourally realistic that individuals actually solve this complicated problem. As noted by \citet{RustML88}, an adapation and learning algorithm may provide a better explanation of how people may find the optimal decision rule in an MDP and thus act \emph{as if} they were utility maximisers. It is possible to find an approximate solution to an MDP using reinforcement learning which some authors has argued represent a good description of how people plan their days \citep[see, e.g.,][]{arentze2004learning}. MDP's of activity-scheduling has been solved using reinforcement learning by \citet{vanhusel09} to model the sequential planning of activities. \citet{Feygin18} proposed in his thesis how the decision rules in an MDP for activity scheduling can be estimated using inverse reinforcement learning. \citet{karlstromScalingUp2009} also investigated how a boltzmann machine can be used to solve an MDP of the DDCM type proposed in this paper. 
 
The paper is structured as follows.
In Section 2 of this paper, we show how a Markov Decision Process (MDP) can be used to model the choice of a daily activity-travel pattern. Following the random utility maximising approach, we assume that individuals maximise the expected achieved utility from a path through activity-location-time-space throughout one day. In the presence of stochasticity, a utility-maximizing agent will derive a decision policy outlining how to act conditional on the outcome of the stochastic process. Such a policy could be interpreted as contingency plan, specifying which travel pattern to choose for each possible combination of outcomes of the stochastic process. This is in contrast to available models where utility-maximisation of a activity-travel pattern has been modelled, e.g., Starchild, MATSim, HAPP and the multistate-supernetwork. One can formulate the choice of an activity-travel pattern as a possible policy where the decisions are fixed before the day start. However, a policy and especially the optimal policy could in general not be defined as a travel pattern, as it typically would not be optimal to act according to a fixed set of decisions independently of what happens. 

%In an MDP these transition probabilities are dependent on the choices of a decision maker and the interest lies in inferring the control law guiding their decisions. This is in in contrast to the Markov chain models discussed above where the interest lies in obtaining transition probabilities conditional on a specific state.

In Section 3, we show how the MDP developed in Section 2 can be formulated as a Dynamic discrete choice model (DDCM) following the work by \citet{rust1987} and thereby give state dependent choice probabilities that are a function not only of the state but also on the expected future utility conditional on the chosen action. We discuss how the expected future utility, i.e., the value function can be approximated using linear interpolation when time is modelled as a continuous variable and when travel time is uncertain. Using the obtained value functions it is possible to calculate choice probabilities which can be used for simulation and maximum likelihood estimation. Following \citet{fosgerau2013}, we also show how the model in a special case degenerates into an MNL model over sequences of actions, and thus can be estimated using sampling of alternatives. Estimates obtained using sampling of alternatives are in contrast to direct maximum likelihood estimates not sensitive to approximations used when calculating the value function.
%While similar to the approach developed in \citet{Habib11RUM}, this expected future utility is not parameterized but instead obtained as the expectation of future one-stage utilities ans is therefore sensitive to changes in the transportation system which only influence the future and so, e.g., the choice of departure time to work in the morning will therefore be sensitive to the traffic conditions in the afternoon. 



In section 4, we present a case study were a model over a working day is specified and estimated on a travel survey. The case study presented in this paper excludes travel time uncertainty, and focus on the special case that can be estimated using sampling of alternatives. By focusing on this special case, we are able to assess the effects of the value function approximation on aggregate predictions. In a separate paper, this case study has been extended with a mixed-logit specificaiton \citep{maelleMixed17}. Section 5 discuss the results and section 6 concludes.


