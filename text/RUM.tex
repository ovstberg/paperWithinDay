In section \ref{seq:mdp} we defined an MDP consisting of a state space, an action space, transition rules and utility functions. We further assumed that individuals act as if they seek a policy which maximise the value function defined by \eqref{eq:vf}. In this section we will add an additional assumption on the distribution $q_\epsilon$ of the state variable $\epsilon$ under which it is possible to derive an analytical expression for the policy which maximise the value function. Observe that value function $V(s)$ in \eqref{eq:vf} can be defined recursively through Bellman's equation as \citep{bellman,Rust87}:
\begin{equation} \label{eq:vfbellman}
\begin{aligned}
V(x_k,\epsilon_k) &= \max_{a_k} \left\{\avgu(x_k,a_k) + \epsilon_k(a_k)+ \eutil(x_k,a_k) \right\} 
\end{aligned}
\end{equation}
where $\eutil(x_k,a_k) $ is the expected value of the value function of the state reached when taking action $a_k$ in state $(x_k,\epsilon_k)$. If $\eutil(x_k,a_k)$ is known for each state-action pair the optimal policy $\pi$ is, by the principle of optimality, to conditional on a state $s_k$ choose the action $a_k$ which maximise the one-stage problem in \refeq{eq:vfbellman}. $\eutil(x_k,a_k) $ is given by: 
\begin{equation} \label{eq:eutil}
\begin{aligned}
	\eutil&(x_k,a_k)  = E_{t\ks{k+1},\amem\ks{k+1},\epsilon_{k+1}}\left[ V(x_{k+1},\epsilon_{k+1}) \mid x_k,a_k \right] \\
	&= \int_{t'} \left( \sum_{j=1}^{N_{\amem}}q_\amem(\amem_j|t', t,\amem,\act{p}) \cdot \ev(x_{k+1}) \right)dq_t(t'|t,l,\act{m},\act{d})).
	\end{aligned}
\end{equation}
where in turn $\ev(x_k) = E_{\epsilon_k} \left[ V(x_k,\epsilon_k) \right]$.
The \emph{expected value function} $\ev(x_k)$ will be used to denote the expected value of the value function \emph{before} the state variables $\epsilon_k$ has been observed.  We will finally assume that $q_\epsilon(\cdot|x_k,a_k)$ is given by the Gumbel distribution and translated to obtain a zero mean. When $\epsilon_k(a_k)$ is i.i.d Gumbel distributed (with zero mean), $\ev$ is given by the log-sum:
\begin{equation} \label{eq:evfbellman}
\begin{aligned}
\ev(x_k) &= %E_{\epsilon_k} \left[ \max_{{a_k}\in C(x_{k})} \left\{\avgu(x_k,a_k) + \mu_{x_k} \epsilon_k(a_k)+ \eutil(x_k,a_k) \right\}  \right] \\&= 
\log\left(\sum_{a_k\in C(x_{k})} e^{\avgu(x_k,a_k) + \eutil(x_k,a_k)}\right).
\end{aligned}
\end{equation}
With i.i.d Gumbel distributed $\epsilon_k$, the probability that an action $a_k$ will be the utility maximising alternative in a state $x_k$ when $\epsilon_{k}$ is unobserved is simply given by the Multinomial Logit Model (MNL): 
\newcommand{\akt}{\tilde{a_k}}
\begin{equation} \label{eq:pcond}
P(a_k|x_k) = \frac{e^{\avgu(x_k,a_k) + \eutil(x_k,a_k)}}{\sum_{\akt \in C(x_k)}e^{\avgu(x_k,\akt) + \eutil(x_k,\akt)}}
\end{equation} 
Other assumptions on the distribution of $\epsilon$ is also possible which would allow for correlation structure and they can, e.g., be assumed to follow a Nested Logit (NL) or Generalised Extreme Value (GEV) distribution \citep{mai2015,mai2016method}.


%Continue here!
\subsection{Solving the value function}
Finding the optimal policy and calculating the choice probabilities using \eqref{eq:pcond} requires that the expected value function is known in each state $x_k$. The equation system consisting of \eqref{eq:eutil} and \eqref{eq:evfbellman} has a solution under regularity conditions outlined in \citet{Rust88}. In infinite horizon models, e.g., the Recursive Logit model for route choice \citep{fosgerau2013}, it is possible to revisit the same state twice and the value function in future states $s_{k+1}$ is thus dependent on the value function in the current state $s_k$. However, as the problem described here has a finite horizon and every action moves forward in time, it is possible to specify the model so that no such loops exists. The expected value function $\ev$ can therefore be calculated by starting at the terminal time step $T$ and moving backward in time. This solution method is often called backward induction. However, it cannot be used directly when time is a continuous variable as there is an infinite number of states. We therefore have to use an approximative solution, which will be discussed below.

\subsubsection{Approximation of the value function}
We propose to approximatively solve the problem by calculating the expected value function in a number of discrete time steps and approximating its value between these times steps using linear interpolation. The time steps will below be denoted $t_j$ and the distance between time steps is $\Delta t$, so that $t_{j+1}-t_j = \Delta t$. There are thus a total of $N_T = T/\Delta t$ time steps where the expected value function is calculated. We will continue to use $\ev(x)$ to denote approximated expected value function which is calculated at these discrete time steps. For the notations below, let $x_{k,t_j}$ specify that $t\ks{k} = t_j$. The values between these time steps is obtained by the function $g(\ev,x_k)$, and these approximate values are used when calculating an approximative solution to $\eutil(x_k,a_k)$: 
\begin{equation} \label{eq:eutilapprox}
\begin{aligned}
\aeutil(x_k,a_k)=
\int_{t'} \left( \sum_{j=1}^{N_{\amem}}q_\amem(\amem_j|t', t,\amem,\act{p}) \cdot g(\ev,x_{k+1}) \right)dq_t(t'|t,l,\act{m},\act{d}))
\end{aligned}
\end{equation}
If linear interpolation is used for approximation $g$ is obtained by the expected value function at the time steps $j-1$ and $j$ where $t_{j-1} \leq t\ks{k} \leq t_j $ as: 
\begin{equation}\label{eq:linap2}
g_\text{linear}(\ev,x_k) = \alpha_1 \ev(x_{k,t_j}) + \alpha_2 \ev(x_{k,t_{j+1}})
\end{equation}
where $\alpha_1 = \frac{t_{j+1}-t}{\Delta t}$ and $\alpha_2 = \frac{t-t_j}{\Delta t}$.
With this approximation, we can give some examples on how stochastic travel time can be implemented.

Let $Q_{\eutil}(x_k,a_k)=Q_{\eutil}(t,l,h,\act{p},\act{m},\act{d})$ return a vector $q$ such that $\eutil(x_k,a_k) =  q \cdot \ev$. 

\paragraph{Example: Uniform travel time}
Assume that there is no transition $\amem$ so that $q_\amem$ can be neglected in \eqref{eq:eutilapprox}. Further assume that the travel time between location $l\ks{k}$ and destination $\act{d}$ is described by a uniform random variable as:
\begin{equation*}
	q_{t,\text{U}}(t'|t,l,\act{m},\act{d})=\begin{cases}
	\frac{1}{b-a} & \cif a \leq t' - t \leq b \\
	0 & \celse
	\end{cases}
\end{equation*}
for some $a$ and $b$ such that $b> a \geq 0$. Let $n_a$ and $n_b$ denote the index of the time step before $a+t\ks{k}$ and after $b+t\ks{k}$ so that $t_{n_a} \leq a+t\ks{k} \leq t_{n_a+1}$ and $t_{n_b-1} \leq b+t\ks{k} \leq t_{n_b}$ respectively. Then:
\begin{equation*}
\begin{aligned}
\aeutil(x_k,a_k)&= \int_{t'} g_\text{linear}(\ev,x_{k+1,t'})dq_{t,\text{U}}(t'|t,l,\act{m},\act{d}))\\
&=\sum_{j=n_a}^{n_b} \beta_j \cdot \ev(x_{k+1,t_j})
\end{aligned}
\end{equation*}
where:
\begin{equation*}
\beta_j = \frac{1}{b-a} \cdot \begin{cases}
1 & \cif n_a +2 \leq j \leq n_{b}-2 \\
0.5 + \frac{t_{n_a+1}-t\ks{k} - a}{\Delta t} & \cif j = n_a +1  \\
\frac{t\ks{k} - a -t_{n_a}}{\Delta t} & \cif j = n_a \\
0.5 + \frac{t\ks{k} - b -t_{n_{b}-1}}{\Delta t} & \cif j = n_{b}-1  \\
\frac{t_{n_b}-t\ks{k} - b}{\Delta t} & \cif j = n_b\\
\end{cases}
\end{equation*}

\subsubsection{Backward induction}
With the general model specification above in mind, we can discuss how the expected value function using backward induction. As discussed before, this calculates the expected value function in each state $x_k$ by moving backward in time, starting at time $t=T$ and ending in $t=0$. Starting the algorithm requires that the value of $\ev$ at time $t=T$ is defined. We assume that the terminal value of a state $x_T$ is given by some function $J_T(x_T) \;\forall x_T \in X_T$, where $X_T$. is the set of states at time $T$. One can define the value at the end of the day so that it is required that an agent returns home before time $T$ or that a specific set of activities $h_T$ is performed during the day by setting $J_T(x_T)=0$ if $l= l_\text{home}$ and $h= h_T$ and $J_T(x_T)=-\infty$ else. The backward induction solution method can now be described by the pseudo-code in Algorithm \ref{alg:backward}. Observe that it is important that $\ev$ is firstly updated in $\tau=0$ before the log-sum of travelling is calculated, as $\ev$ at the destination may be dependent on the value of $\ev$ for $\tau=0$ at the same time step.

%algloop{For}{End}
\begin{algorithm}
\caption{\label{alg:backward} Backward induction algorithm used to calculate the expected value function in all states.}
\begin{algorithmic}[1]
\State $\ev = \text{zeros}(N_\text{T},N_L,N_P,N_\tau,N_M,N_\amem) -\infty $
\For{$x_T \in X_T$} 
\State $\ev[x_T] = J_T[x_T]$
\EndFor

\State
\For{$k \in \{N_\text{T}-1,N_\text{T}-2,\dots,0\}$} 
\For{$m \in M,h \in \{1,2,\dots,N_h\}$,$l \in L,p \in p \in P(h) \text{ s.t } d\in L_\text{act}(P)$}
\State
\State $v_\text{stay} = [\;]$ 
\For{$\tau \in \{0,1,2,\dots,\tau_{max}(p)\}$}
\State $u = u_\text{act}(t_k,p,\tau) + (\tau==0) \cdot  u_\text{start}(l,t_k,h,p)$
\State $\eutil = Q_{\text{stay}}{(t_k,l,m,h,p,\tau,h)} \cdot \ev[:]$
\State $v_\text{stay}[\tau] = \exp(u + \eutil)$
\EndFor
\State $\ev[t_k,l,p,\tau=0,m,h] = \log\left(v_\text{stay}[0]\right)$
\State 
\State $v_\text{travel} = 0$ 
\For{$\act{d} \in L ,\act{m} \in M(m),\act{p} \in P(h) \text{ s.t } d\in L_\text{act}(P)$}
\State $u= u_\text{travel}(t_k,l,m,\act{d},\act{m}) + u_\text{change}(p,\act{p})$
\State $\eutil = Q_{\text{travel}}(t_k,l,m,p,\amem;\act{d},\act{m},\act{p}) \cdot \ev[:]$
\State $v_\text{travel} =v_\text{travel}+ \exp(u+\eutil)$
\EndFor
\State
\For{$\tau \in \{0,1,\dots,\tau_{max}(p)\}$}
\State $\ev[t_k,l,p,\tau,m,h]=\log\big((\tau < \tau_{max}(p))\cdot v_\text{stay}[\tau] + (\tau>0) \cdot v_\text{travel}\big)$
\EndFor
\State
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\subsubsection{Simulating daily travel patterns}
\begin{algorithm}
\caption{Algorithm used to simulate a daily travel pattern}
\begin{algorithmic}[1]
\State $x_0 = \{t=0,l=l_{\text{home}} \}$
\State $x=[x_0]$, $a = []$, $p=0$
\While{$t<T$} 
\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Maximum likelihood estimation}
We have so far suppressed the parameters $\theta$ from the specification of the utility functions, as well as the dependence of the individual $i_n$ that the model describes. 
Suppose that we have observed an individual $i_n$ who have made sequences of actions $\ve{a}_n=\{a_{0,n},a_{1,n},\dots,a_{K_n,n}\}$ and reached states $\ve{x}_n=\{x_{0,n},x_{1,n},\dots,x_{K_n+1,n}\}$. The likelihood for the observation of individual $i_n$ is then given by:
\begin{equation*}
    L_n(\ve{a}_n,\ve{x}_n|x_0,\theta)= \prod_{k=0}^{K_n}  P_n(a_{k,n}|x_{k,n},\theta_u)\cdot q(x_{k+1}|a_{k,n},x_{k,n},\theta_q)
\end{equation*}
Let $N$ observations form the set of observations $\obs_N$. The log-likelihood function for $\obs_N$ based on the conditional likelihoods becomes:
\begin{equation*} \label{eq:cond_like}
\bar{\LL}(\obs_N;\theta) = \sum_{n=1}^N \log \Bl L_n(\ve{a}_n,\ve{x}_n|x_{0,n},\theta)\Bh
\end{equation*}
If we can calculate $L_n(\ve{a}_n,\ve{x}_n|x_{0,n},\theta)$ (and its gradients) we can use any standard optimisation algorithm to find the parameter vector $\theta$ which best explains the observed behaviour. Below we will discuss two ways in which the likelihood function can be obtained, starting with the Nested Fixed Point Method (NFXP), which is the standard method for estimation of dynamic discrete choice models.

\subsubsection{Estimation using NFXP}
%TODO write better!
NFXP relies on using directly using \eqref{eq:pcond} to calculate the conditional choice probabilities $P_n(a_{k,n}|x_{k,n},\theta)$, and is the standard method for estimating dynamic discrete choice models since \citet{Rust87}. To obtain the choice probabilities from, \eqref{eq:pcond} one must first calculate the value of $\eutil$ in all states. As $\eutil$ depends on the parameter values, the value of $\eutil$ (and its gradients) must be updated each time the likelihood-function is calculated for a new parameter vector. Algorithm \ref{alg:backward} must thus be used to obtain $\ev$ for each individual for each trial parameter vector in an optimisation algorithm. Using Algorithm \ref{alg:backward} as an inner algorithm within the optimization algorithm is what constitutes the nested fixed-point problem in the NFXP algorithm. 
\subsubsection{Special case: Estimation using sampling of alternatives}
\label{sec:estSampling}

% 
The fact that $\eutil$ must be calculated using Algorithm \ref{alg:backward} for each trial parameter vector makes the NFXP algorithm computationally demanding. 

As a special case, the probability for a sequence of actions according to a dynamic discrete choice model as in \eqref{eq:pcond} degenerates into an MNL model over sequences of actions \citep{fosgerau2013}. Under these conditions, it can therefore be estimated on a subset of action sequences \citep{mcfadden78}. This equivalence occurs when: 1) each state-action $(x_{k},a_{k})$ pair deterministically gives a new state that thus can be described by a mapping $x_{k+1}=f(x_{k},a_{k})$; and 2) $\epsilon$ is i.i.d Gumbel distributed. Under these restrictions \eqref{eq:vfrust} becomes: $\eutil(x_{k},a_{k})=\ev(x_{k+1})$. Thus:
\begin{align}
P(a_k|x_k) &= \frac{e^{\frac{1}{\mu}\left(\avgu(x_k,a_k) + \ev(f(x_{k+1})\right))}}{\sum_{\akt \in C(x_k)}e^{\frac{1}{\mu}\left(\avgu(x_k,\akt) + \ev(f(x_k,\akt))\right)}} \label{eq:MNL}\\
&=e^{\frac{1}{\mu}\left(\avgu(a_k,x_k) + \ev(x_{k+1})- \ev(x_k)\right)}\label{eq:MNLv2}
\end{align}
where \eqref{eq:evfbellman} together with \eqref{eq:MNL} gives \eqref{eq:MNLv2}. Now consider a day-path, i.e., a sequence of decisions  $\ve{a} = (a_0,...,a_{K-1})$ which starts in a state $x_0$ and traverse the states $\ve{x} = (x_1,...,x_{K})$, where the time at the terminal state $t\ks{K}=T$. The likelihood for this sequence of decisions and states is, according to \eqref{eq:MNLv2} given by:
\begin{equation}\label{eq:pPath0}
\begin{aligned}
P(\ve{a},\ve{x}|x_0) = P(\ve{a}|x_0) &= \prod_{i=0}^{T-1}P(a_i|x_i) \\
& = \prod_{i=0}^{T-1} e^{\frac{1}{\mu}\left(\avgu(a_i,x_i) + \ev(x_{i+1})-\ev(x_i)\right)} \\
& =  e^{\sum_{k=0}^{T-1} \avgu(a_k,x_k) + \ev(x_{K})-\ev(x_0)} 
\end{aligned}
\end{equation}
Assume that the initial state is known and that there is a single state which is feasible at time $T$ denoted by $x_T$, and use the notation $u(\ve{a})=\sum_{k=0}^{T-1} \avgu(a_k,x_k)$. Then the probability of a path $\ve{a}$ among the set $\Agood(x_0,x_T)$ of all action sequences that leads from $x_0$ to $x_T$ is given by:
\begin{equation}
P(\ve{a},\ve{x}|x_0) = \frac{e^{u(\ve{a})}}{\sum\limits_{\ve{a'} \in \Agood(x_0)}e^{u(\ve{a'})} }.
\end{equation}

Estimation on a subset of the choice set involves selecting a choice set $\Ccsamp_n \subset \Cc_n$ and estimating using the conditional choice probability $P_n(\ve{a}_n|\Ccsamp_n)$ instead of the $P_n(\ve{a}_n|\Cc_n)$. A maximum likelihood estimation on a choice set $\Ccsamp$ gives consistent estimates if the correction term $\log(\bar{q}_n(\Ccsamp_n|j))$ is added to each alternative and $\bar{q}_n(\Ccsamp_n|j)$ satisfies the positive conditioning property, i.e., that if $j\in \Ccsamp_n$ and $\bar{q}_n(\Ccsamp_n|i)>0$ for some $i$, then $\bar{q}_n(\Ccsamp_n|j)>0$. This holds if $\Ccsamp_n$ is sampled from the universal choice set $\Cc_n$ and all alternatives in $\Cc_n$ have a non-zero probability of being sampled \citep{mcfadden78}. 

One way of constructing a choice set consists of drawing $R$ alternatives with replacement from the choice set $\Cc_n$ consisting of $J_n$ alternatives, and then adding the observed choice to the choice set. The outcome of such a protocol is $({k}_{n1},{k}_{n2},\dots,{k}_{nJ})$ where ${k}_{nj}$ is the number of times alternative $j$ appears in the choice set, so that $\sum_{j=1}^J {k}_{nj} = R+1$, since the observed alternative $j$ is added once extra to the choice set. Let $q_n(i)$ denote the probability that alternative $j \in \Cc_n$ is sampled. Under this sampling protocol, the estimates are consistent if the correction term $\log(\frac{k_{n\ve{a}_n}}{q_n(\ve{a}_n)})$ is added to each alternative \citep{frejinger09}. After adding the correction term, the likelihood function to use for estimation an individual $n$ becomes:
\begin{equation} \label{eq:cond_like_corr}
\tilde{P}_n(\ve{a}) = \frac{e^{u(\ve{a}) + \log(\frac{k_{n\ve{a}_n}}{q_n(\ve{a}_n)})}}{\sum\limits_{\ve{a}^* \in \Cc_n} e^{u(\ve{a}^*) + \log(\frac{k_{n\ve{a}^*_n}}{q_n(\ve{a}^*_n)})}}
\end{equation}
%\subsection{Solving the value function}
%When modelling daily planning, there is logical terminal time $T$ in the end of the day. We will restrict ourselves to a single feasible state $x_T$ in the end of day with $\eutil(x) = 0$. This is not a restriction per se; multiple states in the end of the day could be included by adding a link from all of these states to a common fictive terminal state $x_k$. With $\eutil(x_k)$ defined, it is possible to use backward induction to calculate $\eutil$ in all states using \refeq{eq:EV}.
%
%
%- Continuous time 
%
%\subsection{Overlapping paths}
%In the graph describing the daily activity-travel network, multiple possible day-paths of sequences of states and actions will pass through the same state-action pair $(x,a)$. It is reasonable to assume that these day-paths share some portion of the unobserved utility component. In route choice models this is a well known problem, and there is a risk that not taking it into account will give unrealistic substitution patterns. To overcome this problem without explicitly introducing an unobserved link specific random utility component, it is common to add some deterministic correction term to the utility of a path which models the degree of overlap with other paths in the choice set. Common examples in the route choice literature include the Path Size Logit (PSL) \citep[][]{} and C-Logit \citep{}. As noted in \citet{fosgerau2013}, these correction terms are not additatively separable over links and can therefore not used in a model where choices are made sequentially over links, and they can therefore not directly be used here. In the context of route-choice, \citet{fosgerau2013} developed an alternative correction term which they called the Link Size (LS) attribute. The LS-attribute is derived by calculating the expected flow on each link according to some auxiliary model. They show that LS-attribute gives substitution patterns very similar to the PSL.
%
%The probability that a specific state-action pair will be observed is given by:
%\begin{equation}
%P(x,a) = P(x)\cdot P(a|x)
%\end{equation}
%where $P(a|x)$ are given by \eqref{eq:pcond} and where the $P(x)$ is the probability that an individual reaches state $x$ and is given by the solution to the equation system:
%\begin{align}
%P(x_{k+1}) &= \sum_{x_k \in X} \sum_{a_k \in C(x_k)}q(x_{k+1}|a_k,x_k)\cdot P(a_k|x_k)\cdot P(x_k) \\
%&=\sum_{x_k \in X} \sum_{a_k \in C(x_k)}q(x_{k+1}|a_k,x_k) e^{\frac{1}{\mu_{x_k}}\left(\avgu(x_k,a_k) + \eutil(x_k,a_k)\right) - \ev(x_k}
%\cdot P(x_k)
%\end{align}
%As all actions lead forward in time, it is clear that the stationary probabilities can be obtained by starting at $t=0$ and moving forward in time once $P(a|x)$ has been calculated. 
%\begin{equation}
%P(x_{k+1}) = \sum_{x_k \in X} \sum_{a_k \in C(x_k)}q(x_{k+1}|a_k,x_k)\cdot \cdot P(x_k).
%\end{equation}
%
