A state should include the information needed to determine available actions and utility of these actions. Here a state $x$ consists of:
\begin{description}[style=multiline,leftmargin=4cm,font=\normalfont]
\item[{Time $t \in [5\unit{am},11\unit{pm}] $}:] Continuous variable for time of day. A day starts at $5\unit{am}$ and ends at $11\unit{pm}$
\item[{Location $L \in [1,1240]$:}] Current location. One of 1240 zones in the region of Stockholm. 
\item[Activity $A$:] New activity, end activity, social, recreational, shop, home, work and escorting children are the alternative activity states. 

The activity must be included in the state since the individual can choose to continue with the same activity for yet another time-period, but have to travel (possibly within the zone) to change activity. The purpose of the new activity and end activity states will be discussed below.
\item[{Errand indicator $E \in [0,3]$:}] A state keeping track of the number of finished mandatory activities. The number of mandatory activities varies from 1 to 3 depending on the individual, as will be explained later. 
\item[Car dummy $\delta_{car} \in \{true,false\}$:] Dummy for car availability. An individual have to travel with car if $\delta_{car} = true$ and if out of home and cannot travel with car if $\delta_{car} = false$.
\end{description}


The set of actions $a$ that are available in a state $x$ for individual $n$ is denoted $A_n(x)$. The universal choice set consists of any combination of activity $A$, mode $M$ and location $L$:
\begin{description}[style=multiline,leftmargin=4cm,font=\normalfont]
\item[{Activity $A$}:] Activity for new action.
\item[{Location $L$}:] New location. 
\item[{Mode}:] Car, public transport, bike and walk are the modelled modes. When continuing with the same activity, the mode of the action is ``no-mode''.
\end{description}


When starting a new activity with flexible duration it is initially conducted for one time-step, which we have chosen to be $10\unit{minutes}$. Depending on the activity and on time-space constraints it can be possible to continue with the same activity for another time step. The action-space is thus discrete and finite. This means that every 10th minute individuals can decide whether to continue with the current activity for another $10\unit{minutes}$. Travel times are not divisible by these time step lengths, and it therefore makes sense to have a continuous state variable for time. Since there are a finite number of actions in each state there will still only be a finite number of reachable states. It is sometimes argued that activity length should be a continuous variable, as it is included in, e.g., \citep{Habib11RUM}, \citep{Pinjari10} and \citep{Recker13}, but from a behavioural perspective we think it makes at least as much sense to assume that people are considering whether to, e.g., spend 10, 20 or $30\unit{minutes}$ shopping as to assume that they decide to spend exactly $17.3123\unit{minutes}$. 

%The alternative activities (or trip purposes) $A$ are work, home, escort children to/from school, grocery shopping, social, recreational and other. Shopping is in turn divided into three categories: small, medium and large. There is thus a total of 9 alternative activities. 

Work and child errands have fixed duration (10 minutes for dropping off children and working hours as observed). The remaining alternatives can be continued for any number of time steps. Most computations come from calculating the log-sums in \refeq{eq:EV} for all states, and as activities and locations are both states and alternatives in each state, the computational time will increase quadratic with both the number of locations and the number of activities. To reduce computational time, the ``start-activity'' and ``end-activity'' states are added. The reason for this can be illustrated with an example. In each state, an individual can choose to either continue with the same activity or start a new activity at any location. This gives $1 + N_{act}\cdot N_{m}\cdot N_{loc}$ alternatives, and calculating $\eutil$ in a state therefore requires summing up $1 + N_{act}\cdot N_{m}\cdot N_{loc}$ factors. In each time step, there are approximately $N_{act}\cdot N_{loc}$ states for which this operation is performed. In a ``start-activity''-state, the only available alternatives are to start one of the available activities, so there are approximately $N_{act}$ factors that needs to be summed together. Once we have $\eutil$ in this state, we can divide the choice of a new activity into two steps, firstly the choice of a new location and mode, and secondly the choice of activity. This will not change the choice probability when they are given by \refeq{eq:MNL}. With this new state, the number of terms reduces to $1 + N_{m}\cdot N_{loc}$ (where 1 is the alternative to continue with the same activity), and approximately decreases with a factor $N_{act}$. This comes at the expense of calculating the log-sum of $N_{act}$ in $N_{loc}$ states. When considering a new action, the future utility is independent of the current activity, and it is therefore possible to create an ``end-activity'' where the sums of all possible ``start-activity'' states is calculated. Instead of having to sum up $1 + N_{act}\cdot N_{m}\cdot N_{loc}$ factors in $N_{act}\cdot N_{loc}$ states we sum up the $N_{m}\cdot N_{loc}$ ``start-activity'' factors in $N_{loc}$ ``end-activity'' states, so the computational savings can be significant. This also means that increasing the number of activities only have a minor effect on the computation time.