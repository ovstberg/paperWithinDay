In this section we will discuss in more detail how to specify a model for the choice of a daily travel pattern using an MDP. Remember that an MDP is defined by a tuple $(S,C,q,u)$ where: $S$ is the state space; $C$ is the set of decision for which there exist a function $C(s)$ specifying the set of available actions in a specific state; $q$ is the transition probabilities defining the probability to reach a new state reached when taking a specific action in a state; and $u$ is the one-stage utility function defining the immediate reward obtained from a specific state-action pair.

\subsection{Actions and states}
The decision variables which defines an actions $a_k$ are: destination $\act{d} \in L$; mode of transport $\act{m} \in M$; and purpose $\act{p} \in P$. Here $L$, $M$ and $P$ defines the set of locations, modes and purposes respectively.  The destination variable $d$ stores the index of the location among $N_L$ possible locations, so $d \in  L = \{1,2,\dots, N_L\}$. The set of modes can contain, e.g., car, public transport, walk and bike, or potentially any combination of such modes that can be used to travel between a specific origin-destination pair. Besides these modes, the set will also contain an option for staying in the current location. The number of modelled modes is denoted $N_M$ and so the set of modes is given by: $\act{m} \in M = \{m_{\text{stay}},m_1,\dots,m_{N_m}\}$, where $m_{\text{stay}}$ denotes the option to stay. We define three different types of purposes: 1) to participate in one out of $N_{act}$ activities $(p\in P_{act} = \{p_1,\dots,p_{N_P}\})$.; 2) to travel ($p=p_{\text{travel}}$) in which case the purpose of $a_{k+1}$ must be to participate in an activity $p \in P_{act}$; and 3) to end an activity ($p=p_{\text{end}}$), in which case the purpose of the $a_{k+1}$ must be to travel. The total set of purposes $P$ is thus given by $P = \{p_{\text{travel}},p_{\text{end}},p_1,\dots,p_{N_P}\}$

The action $a_k$ can then be written as: 
\begin{equation} 
    a_k = \begin{pmatrix}
    \act{d} \\
    \act{m} \\
    \act{p}    
    \end{pmatrix} =
    \begin{pmatrix*}[l]
    \text{destination} \\
    \text{mode-of-transport} \\
    \text{purpose} \\
    \end{pmatrix*} 
    \subset
    \begin{Bmatrix*}[l]
    \{1,2,\dots, N_L\}, \\
    \{m_{\text{stay}},m_1,\dots,m_{N_m}\}, \\
    \{p_{\text{travel}},p_{\text{end}},p_1,\dots,p_{N_P}\} 
    \end{Bmatrix*}=
    \begin{Bmatrix*}[l]
    L \\
    M \\
    P \\
    \end{Bmatrix*}
\end{equation}

We know turn to the definition of a state. A state $x_k$ must firstly consist of the current location $l \in L$ and time-of-day $t$ which is modelled as a continuous variable between 0 and $T$, i.e., $t\in [0,T]$. A number of state variables can then be defined to represent the part of the history which may influence future choices. 

A state variable $p \in P$ will be used to define the purpose of the action leading to the current state. The availability of certain modes of transport (especially car and bike) to an individual will be dependent of previously modes used during a day. Especially, that a car was used on the previous trip on a tour will indicate that the individual has a car available in their current location. We therefore include a state variable $m$ which is dependent on the last used mode of transport. 

To represent previously performed activities in the state space we will firstly distinguish between two different types of activities. The first type consist of mandatory activities for which the time-space constraints are such that only a single internal order is possible. It may be that an individual during a day must both drop-of and pick-up children at school as well as go to work. With normal school and working hours, these activities must be performed in the order: drop-of, go to work, pick-up. The number of such ordered mandatory activities which has been performed during a day will be stored in the state variable $o$. For all activities that does not belong to that are not ordered mandatory, a state variable $\amem$ will be used to remember the number of times each activity has been performed. In principle, $\amem$ could further be divided into one state variable for each activity. Finally, the state $x_k$ is given by:
\begin{equation}
    x_k = \begin{pmatrix}
    t \\
    l \\
    p \\
    \atime_p\\
    m \\
    o \\
    \amem
    \end{pmatrix} =
    \begin{pmatrix*}[l]
    \text{time-of-day} \\
    \text{location } \\
    \text{purpose of previous action} \\
    \text{time spent on purpose $p$} \\
    \text{previous/main mode of transport} \\
    \text{\# performed ordered mandatory activities} \\
    \text{activity memory} \\
    \end{pmatrix*}
    \subset
    \begin{Bmatrix*}[l]
    T \\
    L \\
    P \\
    \Tau \\
    M \\
    O \\
    \Xi 
    \end{Bmatrix*}
\end{equation}

\subsection{Conditional choice sets}


Defining the choice set $C(x_k)$ involves defining the set of purposes, destinations and modes that are available in a specific state $x_k$.
In principle, an individual can in each time step decide between: either staying at the same location and perform the previous activity $p$ for a while longer; or travel to another location and start a new activity there. In the first case, we assume that each activity purpose $p$ has a minimum additional duration $\Delta_p t$. When the chosen alternative is to stay and continue with the activity, it will be performed for an additional $\Delta_p t$.
The availability of activities are limited in time and space. Due to such time-space constraints it may not be possible to continue with an activity after a certain time, and it is in general not possible to start any activity at any location. For example, home and work activities are restricted in space to the home and work location, and the work activity is potentially restricted in both duration and time-of-day by the individuals working schedule. One can view such time-space restrictions as location specific opening hours for each activity: an activity with purpose $p$ can only be performed at location $l$ between the opening time $t^{\text{open}}({l,p})$ and closing time $t^{\text{close}}({l,p})$. Certain activities, e.g., work, may also have a minimum and maximum duration for each episode. The minimum duration for a purpose $p$ will be denoted $\tau^{min}(p)\geq \Delta_p t$ and the maximum duration $\tau^{max}(p)\leq T$. 

If $p=p_\text{travel}$, it means that the agent travelled to the current location with the previous action with the purpose to start a new activity in the current decision stage. The mode and destinations choice is therefore restricted to $\act{m} = m_{\text{stay}}$ and $\act{d}=l$. The set of available purposes is determined by the time space constraints as:
$P(t,l) = \big \{ p \in P_{act} \;\text{s.t:} \; t^{\text{open}}({l,p}) \leq t \leq t^{\text{close}}({l,p}) - \tau^{min}(p) \big \} $, i.e., it must be possible to start the activity and pursue the activity for its minimum duration $\tau^{min}(p)$ within the opening hours at the specific location.



If $p=p_\text{end}$, the agent has ended an activity with the previous action in order to travel in the current decision stage, so $\act{p}=p_\text{travel}$. The available alternatives thus consist of any combination of destination $\act{d} \in L$ and mode $\act{m}\in M(m)\subset M$ where the available modes $M(m)$ is assumed to only depend on the state variable $m$. 


If $p\in P_{act}$, the agent is currently performing an activity and can either continue with the same activity of another time step or end the activity and travel in the upcoming decision stage. It is possible to continue with an activity as constraints on maximum duration and opening hours are satisfied, i.e., if $t+\Delta_p t \leq t^{\text{close}}({l,p})$ and $\tau_p + \Delta_p t \leq \tau^{max}(p)$

\begin{align}
    C(x_k) = \left \{
    {\def\arraystretch{2}\tabcolsep=2pt
    \begin{tabular}{llllccl}
  \Big\{ & $\act{d}=l$,             &$\act{m}= m_\text{stay}$,   &$\act{p} \in P(t,l)$             &\Big\}  &if&  $p=p_\text{travel}$\\
  \Big\{ & $\act{d} \in L$,         &$\act{m} \in M(m)$,         &$\act{p}=p_\text{travel}$        &\Big\}  &if&  $p = p_{\text{end}}$ \\
  \Big\{ & $\act{d} = l$,           &$\act{m}=m_\text{stay}$,    &$\act{p} \in \{p,p_\text{end}\}$ &\Big\}  &if&  $p \in P_{act}$ \text{and} $t+\Delta_p t \leq t^{\text{close}}({l,p})$ \\
  \Big\{ & $\act{d}=l$,             &$\act{m}=m_\text{stay}$,    &$\act{p} = p_\text{end}$         &\Big\}  &if& $p \in P_{act}$ \text{and} $t+\Delta_p t > t^{\text{close}}({l,p})$ \\
  \end{tabular}
  }
    \right.
\end{align}
\subsection{Transition probabilities}
The state $x_{k+1}$ reached when taking action $a_k$ in state $x_k$ is a possibly random variable given by the probability density function $q(x_{k+1}|x_k,a_k)$. To distinguish between the state variables in state $x_{k+1}$ and $x_k$ we will use the notation $i\ks{k}$ and $i\ks{k+1}$ for state variable $i$ in decision stage $k$ and $k+1$ respectively.
The transitions of the state variables $l\,p,\,\atime_p,\,m$ and $o$ are deterministic given the state-action pairs. We will use $f_{i}$ to denote this transition function for respectively state variable $i$. 

\begin{equation}
\begin{aligned}
t\ks{k+1} & \sim q_t (\cdot | t\ks{k},l\ks{k},\act{m},\act{d}) \\
l\ks{k+1} &= \act{d} \\
p\ks{k+1} &= \act{p} \\
\atime_p\ks{k+1} &= f_{\atime_p}(\atime_p\ks{k},\act{p}) \\
m\ks{k+1} &= f_m(m\ks{k},\act{m}) \\
o\ks{k+1} &= f_o(o\ks{k},\act{p}) \\
\amem\ks{k+1} & \sim q_\amem\left(\cdot | t\ks{k}, t\ks{k+1},\amem\ks{k},\act{p}\right) 

\end{aligned}
\end{equation}


and let $\indicator{A}(X)$ denote the indicator function:
\begin{equation}
\indicator{A}(X) =
\begin{cases}
1 \cif X\in A \\
0 \celse
\end{cases}
\end{equation}


We assume that the joint probability density function for respectively state variable is separable as:

\begin{equation}
    q(x_{k+1}|x_k,a_k) = \begin{pmatrix}
    q_t(t'|t,\act{m},l,d) \\
    \indicator{\act{d}}(l')  \\
    \indicator{\act{p}}(p') \\
    \indicator{f_p(\atime_p,\act{p})}(\atime_p')\\
    \indicator{f_m(\hat{m},m)}{(m')} \\
    o' \\
    \amem'
    \end{pmatrix} =
\end{equation}



\begin{equation}
 a_k \times x_k =  \begin{pmatrix}
    \act{d}\ks{k} \\
    \act{m}\ks{k} \\
    \act{p}\ks{k}    
    \end{pmatrix} \times \begin{pmatrix}
    t\ks{k} \\
    l\ks{k} \\
    p\ks{k} \\
    \atime_p\ks{k}\\
    m\ks{k} \\
    o\ks{k} \\
    \amem\ks{k}
    \end{pmatrix} \Rightarrow \begin{pmatrix}
    t'\\
    \act{d}\ks{k} \\
    \act{p}\ks{k} \\
    f_{\atime_p}(\atime_p\ks{k},\act{p)\ks{k})}\\
    f_m(m\ks{k},\act{m}\ks{k}) \\
    f_o(o\ks{k},\act{p}\ks{k}) \\
    \amem'
    \end{pmatrix}
\end{equation}
where $t'$ and $\amem'$ are random variables defined by the probability density functions $q_t(\cdot|t,m,\act{d},l)$ and $q_\amem(\cdot|t,\act{p},\tau_p,\amem)$ respectively.

\subsection{One-stage utility functions}