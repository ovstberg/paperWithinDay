When individuals decide what time to leave for work, they take into consideration how this will influence it will influence their afternoon. If they arrive ten minutes later it means that they have to spend less time at work and/or less time performing activities in the afternoon. The difference in how they value afternoon-time versus morning-time will therefore influence their departure time for work. Even people with flexible working hours seem to prefer to go to work during rush hours, when roads and public transport are heavily congested. Probably they have other time-constraints -- they might have to pick up their children from school, have a gym-class or are meeting friends -- or time preferences -- they might value having dinner with their family at $6\unit{p.m}$. Whatever the reasons, it is clear that the timing of trips in the morning is influenced by their plans for the afternoon, and that the benefits of travelling at less congested times would not outweigh the costs of changing these plans. The decision on when, where and how to travel should therefore be explained by trade-off's on how a limited amount of time should be spent and a correct representation of time is crucial in activity-based travel demand models. 

The importance of modelling the choice of a daily travel pattern interdependently has been argued by multiple authors \citep[see, e.g.,][]{recker86starchild1,kitamura1996Sams}. A natural way to represent this is by assuming that individuals act as if they were utility maximizers, and choose the utility maximising travel pattern $tp$ among the set $TP$ of all feasible travel patterns:
\begin{equation}
\label{eq:opt}
	\begin{aligned}
	& \underset{tp}{\text{maximize}}
	& & U_{tp} \\
	& \text{subject to}
	& & tp \in TP
\end{aligned}
\end{equation}
Formulating a model based on \eqref{eq:opt} is by no means trivial, and several models have been proposed based on this assumption, see e.g., \citet{Adler79}, MATSim \citep{horni2016multi}, AURORA \citep{joh2003Aurora, johEstimationAurora2005}, HAPP \citep{recker01bridge,Recker13,yuan2014HAPP}. 

\subsection{MDP}
\label{seq:mdp}
In this section we will discuss how a sequence of decisions formulated as a Markov decision process (MDP) can be used to model the choice of a daily travel pattern. The decision sequence will include the choice of the number of trips to perform during a day as well as their their timing, destination, purpose and mode of transport. One could conceptually extend such a decision sequence to incorporate route choice in a multi-modal network but that is not the focus of this paper.
% It is simple to represent a travel pattern as a set of states and decisions.
% However, in a markov decision process, it is necessary that the relevant part of the history is also caputred by $s$. On the other hand, in order to obtain an operational model, some part of the history has to be excluded from the state variable. One such part which would be extremely hard to include is previously visited locations. 
% Location, $l$, as this influence. 
% Performed activites or activities which are required to be performed during a day. 
% In this way, it is possible to model time-space constraints. 

% One could imagine stock variables which influences the need to perform certain activities during a day. 
% There is also a need for variables keeping track of the mode used on the tour. 
% 
% In \citep{Arentze04,liao13}, the fact that multiple states keep track of previously performed activities motivates the name \emph{multi}-state supernetwork.
% In each such state 

% The markov property comes with a number of limitations compared to xxx. It becomes very hard to keep track on the time spent on each activity or with each model during a day. However, the most commong implementation of XX is as sum_Utravel + sum Uact and this can included

\newcommand{\bs}{\mathbf{s}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\avgu}{u}

The choice of a daily travel pattern is represented by a sequence of actions forming a path between states where a state $s_k$ may define the location $l$ and time of day $t$ among other things. The set of all states is denoted by $S$ and the set of states available at a specific point in time $t$ for $S_t$. We will let the time $t$ be a continuous variable but assume that decisions are taken at discrete points in time. The index $k$ is used to denote the order of the state $s_k$ in the sequence of states that are traversed during the day. In each state, an individual can choose an action $a_k \in C(s_k)$ where $C(s_k) \subset C$ defines the subset of discrete actions which are feasible in the specific state $s_k$. An action may define, e.g., type and duration of an activity or destination and mode of transport of a trip. 

If the environment is stochastic, the state $s_{k+1}$ reached when choosing $a_k$ in state $s_k$ may be uncertain and given by some probability density function $q(s_{k+1}|a_k,s_k)$. Such uncertainty may be caused by daily variations in travel times. It may also be that a friend suddenly calls, that a meeting is cancelled or that a family member is expected to call and ask for a lift. One could have a state which represents the need to perform an activity on a specific day and allow the stochastic process $q$ to model how this need evolves over a day. This would resemble how, e.g., the need based models of \citet{arentze11} captures how the need to perform activities evolve between days over a week. 

We assume that individuals are aware of the stochasticity introduced by $q$ and take it into account when making decisions. The agents preferences for taking a decision $a_k$ in a specific state $s_k$ and reaching the state $s_{k+1}$ is represented by the one-stage utility function $u(s_k,a_k,s_{k+1})$. The utility function is dependent on factors such as the travel time (which is dependent on $s_k$ and $s_{k+1}$), travel cost, the time spent performing different activities and the time-of-day when they take place. Together the Markov decision process consists of: the state space $S$; decision space $C$ and constrained choice set $C(s_k)$; transition probabilities $q$; and one stage utility functions $u$. A travel pattern $tp$ can be defined by the sequences $\bs$ and $\ba$ of actions and states traversed during a day respectively. Similarly to the assumption of \eqref{eq:opt} we will assume that an individuals preference for an experienced travel pattern is given by the utility function $U(\bs,\ba)$:
\begin{equation}
    U(\bs,\ba) = \sum_{k=0}^K u(s_k,a_k,s_{k+1}).
\end{equation}
% sequence of such state-action pairs $\{s_k,a_k\}$ starting in the morning and ending in the evening is what we will call a day-path. A day-path defines a travel pattern or schedule but further contains information of the sequence of actions and states that have been traversed.
A rational agent that starts in a state $s$ would behave according to a policy $\pi$, determining the action $a_k=\pi(s_k)$ to take when in state $s_k$, that maximizes the expected future utility  of a day. The expected future utility conditional on a state is the \emph{value function} in that state which is defined by: 
\begin{equation}\label{eq:vf}
\begin{aligned}
V(s) & = \max_{\pi} E_\pi \left\{ \sum_{k=0}^K u(s_k,a_k,s_{k+1})\middle | s_0 = s \right\}  
\end{aligned}
\end{equation}
where the expectation $E_\pi$ is with respect to the stochasticity of $s_k$ give the decision rule $a_k=\pi({s_k})$ and $K$ is the maximum number of decision stages in a day. Observe that since individuals are assumed to consider the expected value of one-stage utilities, one can without loss of generality exchange $u(s_k,a_k,s_{k+1})$ for $\avgu(s_k,a_k) = E_{a_k}[u(s_k,a_k,s_{k+1})]$, and we will in the future use this expected one-stage utility to simplify the notation. However, in the context of activity scheduling with stochastic travel time, the one-stage utility will in general be dependent on the stochastic process (if travel time influences utility).

Observe that the transition probabilities $q$, the one-stage utility function $u$ and choice-set $C(s_k)$ are assumed to be Markovian: conditional on $s_k$ and $a_k$ they are independent of the history. This means that a state should include all the information necessary to formulate the choice set and one-stage utility functions. The Markov assumption is not a problem in theory, as the state could be defined to all previous history. However, due to the curse of dimensionality it is in practice important to limit the part of the history which is stored in a state in order to reduce the size of the state space $S$ and obtain a tractable model. For example, a state variable can be introduced for mode of transport used on a previous trip, but it would be extremely complicated to introduce a state variable for the total number of minutes spent travelling with each mode. It is possible to introduce a state variable related to the type of activities that has been performed during a day, but it would not be feasible to also remember \emph{where} they were performed. The fact that, e.g., previous location choices are excluded from the state space does however not mean that they are independent. Where you have been influences where you are, what you have done and how much time you have left for other activities, all of which may influence your future destinations. 

It can at this point be worth comparing the assumptions underlying \eqref{eq:opt} and \eqref{eq:vf} and the relationship between the choice of a travel pattern and a policy. Whereas \eqref{eq:opt} implies that individuals search for the optimal travel pattern $tp$, potentially for the expected utility of a day, \eqref{eq:vf} implies that individuals search for an optimal decision rule $a = \pi(s)$ for each state $s$. One can formulate the choice of a travel pattern $tp$ as a possible policy where the decisions are fixed before the day start. However, a policy and especially the optimal policy $\pi$ could in general not be defined as a travel pattern, as it does not necessarily contain a fixed set of decisions independently of what happens. Instead, a policy is a set of condition-action pairs, determining how an individual should act conditional on the experienced history as represented by the state. In a way, a policy could be interpreted as contingency plan, specifying which travel pattern to choose for each possible combination of outcomes of the stochastic process.


\newcommand{\s}[1]{s_{#1}}
We will in the following discuss in more detail how to specify a model for the choice of a daily travel pattern using an MDP. As discussed above, an MDP is defined by a tuple $(S,C,q,u)$ where: $S$ is the state space; $C$ is the set of decision for which there exist a function $C(s)$ specifying the set of available actions in a specific state; $q$ is the transition probabilities defining the probability to reach a new state reached when taking a specific action in a state; and $u$ is the one-stage utility function defining the immediate reward obtained from a specific state-action pair. We will in the following describe how respectively part of the MDP is defined, starting the definition of an action $a_k$ and the set of possible decisions $C$.
\subsection{Actions}
The decision variables which defines an action $a_k$ are: destination $\act{d} \in L$; mode of transport $\act{m} \in M$; and purpose $\act{p} \in P$. Here $L$, $M$ and $P$ defines the set of locations, modes and purposes respectively.  The destination variable $d$ stores the index of the location among $N_L$ possible locations, so $d \in  L = \{1,2,\dots, N_L\}$. The set of modes can contain, e.g., car, public transport, walk and bike, or potentially any combination of such modes that can be used to travel between a specific origin-destination pair. Besides these modes, the set will also contain an option for staying in the current location. The number of modelled modes is denoted $N_M$ and so the set of modes is given by: $\act{m} \in M = \{m_{\text{stay}},m_1,\dots,m_{N_m}\}$, where $m_{\text{stay}}$ denotes the option to stay. We define three different types of purposes: 1) to participate in one out of $N_{act}$ activities $(p\in P_{act} = \{p_1,\dots,p_{N_P}\})$, where $p_\text{home}$ is an important special case; 2) to travel ($p=p_{\text{travel}}$) in which case the purpose $\act{p}$ of $a_{k+1}$ must be to participate in and thus start an activity $p \in P_{act}$; and 3) to end an activity ($p=p_{\text{end}}$), in which case the purpose of the $a_{k+1}$ must be to travel. The total set of purposes $P$ is thus given by $P = \{p_{\text{travel}},p_{\text{end}},p_1,\dots,p_{N_P}\}$
To summarise, an action $a_k$ can be written as: 
\begin{equation} 
    a_k = \begin{pmatrix}
    \act{d} \\
    \act{m} \\
    \act{p}    
    \end{pmatrix} =
    \begin{pmatrix*}[l]
    \text{destination} \\
    \text{mode-of-transport} \\
    \text{purpose} \\
    \end{pmatrix*} 
    \subset
    \begin{Bmatrix*}[l]
    \{1,2,\dots, N_L\}, \\
    \{m_{\text{stay}},m_1,\dots,m_{N_m}\}, \\
    \{p_{\text{travel}},p_{\text{end}},p_1,\dots,p_{N_P}\} 
    \end{Bmatrix*}=
    \begin{Bmatrix*}[l]
    L \\
    M \\
    P \\
    \end{Bmatrix*}
\end{equation}
The set of all possible actions $C$ is given by all possible combinations of $L$, $M$ and $P$ and the total number of alternatives is denoted by
$N_C$.
\subsection{States}
 A state $\s{k}$ must firstly consist of the current location $l \in L$ and time-of-day $t$ which is modelled as a continuous variable between 0 and $T$, i.e., $t\in [0,T]$. A number of state variables can then be defined to represent the part of the history which may influence future choices. 
A state variable $p \in P$ will be used to define the purpose of the action leading to the current state. If the utility of performing an activity is dependent on the duration that is has been performed or if an activity has a maximum utility, it will be necessary to remember the amount of time that has passed since it was started. The state variable $\atime \in \{0,1,2,\dots,N_{\atime,p} \}$ stores the number of times the individual has chosen to continue the current activity $p$ since it was first started, where $N_{\atime,p}$ is the maximum memory for purpose $p$. The maximum value of $N_{\atime,p}$ for any purpose $p$ is denoted $N_\atime$. The availability of certain modes of transport (especially car and bike) to an individual will be dependent on previous modes used during a day. Especially, if a car was used on the previous trip on a tour, it will indicate that the individual has a car available in their current location. The state variable $m \in M$ is used to store information related to the available modes of transport. It can, for example, define the main-mode of a tour or the mode of transport used on the last travel episode.
A state variable $\amem$ stores the relevant history related to previously performed activities. This variable could in principle store the number of times each activity has been performed during the day. It could also store information about the need to perform different activities. Let the maximum number of such different histories and/or need combinations that are modelled be $N_\amem$. Then $\amem\in\{\amem_1,\amem_2\dots,\amem_{N_\amem} \}$. Finally, we let a state vector $\epsilon \in \rr^{N_C}$ represent the stochastic and non-modeled random attributes of the available actions. In total, the state $\s{k}$ is given by:

\begin{equation}
    \s{k} = \begin{pmatrix}
    t \\
    l \\
    p \\
    \atime\\
    m \\
    \amem \\
    \epsilon
    \end{pmatrix} =
    \begin{pmatrix*}[l]
    \text{time-of-day} \\
    \text{location } \\
    \text{purpose of previous action} \\
    \text{time spent on current purpose $p$} \\
    \text{previous/main mode of transport} \\
    \text{activity history} \\
    \text{random state vector}
    \end{pmatrix*}
    \subset
    \begin{Bmatrix*}[l]
    [0,T] \\
    \{1,2,\dots, N_L\} \\
    \{p_{\text{travel}},p_{\text{end}},p_1,\dots,p_{N_P}\} \\
    \{0,N_{\atime}\} \\
    \{m_{\text{stay}},m_1,\dots,m_{N_m}\} \\
    \{\amem_1,\amem_2\dots,\amem_{N_\amem} \}\\
    \rr^{N_C}
    \end{Bmatrix*}
\end{equation}
In the future, we will also use the notation $x_k=(t,l,p,\atime,m,\amem)$ to denote the observable part of the state space.
\subsection{Conditional choice sets}
Defining the choice set $C(\s{k})$ involves defining the set of purposes, destinations and modes that are available in a specific state $\s{k}$.
In principle, an individual can in each time step decide between: either staying at the same location and perform the previous activity $p$ for a while longer; or travel to another location and start a new activity there. In the first case, we assume that each activity purpose $p$ has a minimum additional duration $\Delta_p t$. When the chosen alternative is to stay and continue with the activity, it will be performed for an additional $\Delta_p t$.
The availability of activities are limited in time and space. 
Due to such time-space constraints it may not be possible to continue with an activity after a certain time. Further, certain activities may be restricted to a subset of the locations. For example, home and work activities are restricted in space to the home and work location, and the work activity is potentially restricted in both duration and time-of-day by the individuals working schedule. One can view such time-space restrictions as location specific opening hours for each activity: an activity with purpose $p$ can only be performed at location $l$ between the opening time $t^{\text{open}}({l,p})$ and closing time $t^{\text{close}}({l,p})$. Certain activities, such as work, may also have a minimum and maximum duration for each episode. The minimum duration for a purpose $p$ will be denoted $\atime^{min}(p)\geq \Delta_p t$ and the maximum duration $\atime^{max}(p)\leq T$. 
When $p=p_\text{travel}$, the agent travelled in the previous action ($a_{k-1}$) but have yet to start an activity. The activity history $\amem$ may influence the activity that an individual can start. This allows us to put restrictions on the number of times an activity can be performed each day. The purpose of action $a_k$ is therefore restricted to one of the activities, i.e., $\act{p} \in P_{act}(\amem)$. As no travelling takes place, mode and destinations choices of $a_k$ are restricted to $\act{m} = m_{\text{stay}}$ and $\act{d}=l$. The set of available purposes is determined by the time space constraints as:
$P(t,l,\amem) = \big \{ p \in P_{act}(\amem) \;\text{s.t:} \; t^{\text{open}}({l,p}) \leq t \leq t^{\text{close}}({l,p}) - \atime^{min}(p) \big \} $, i.e., it must be possible to start the activity and pursue the activity for its minimum duration $\atime^{min}(p)$ within the opening hours at the specific location.



If $p=p_\text{end}$, the agent has ended an activity with the previous action in order to travel in the current decision stage, so $\act{p}=p_\text{travel}$. The available alternatives thus consist of any combination of destination $\act{d} \in L$ and mode $\act{m}\in M(m)\subset M$ where the available modes $M(m)$ is assumed to only depend on the state variable $m$. 


If $p\in P_{act}$, the agent is currently performing an activity and can either continue with the same activity for another time step or end the activity and travel in the following decision stage. It is possible to continue with an activity as constraints on maximum duration and opening hours are satisfied, i.e., if $t+\Delta_p t \leq t^{\text{close}}({l,p})$ and as long as $\atime_p + \Delta_p t \leq \atime^{max}(p)$. In summary, the set of actions available conditional on a state $s_k$ is given by:
\newcommand{\tableexp}[1]{{{\footnotesize\emph{#1}}}}
\begin{align} \label{eq:choiceset}
    C(\s{k}) = \left \{
    {\def\arraystretch{1.4}\tabcolsep=2pt
    \begin{tabular}{llllccl}
        \multicolumn{5}{c}{\tableexp{start activity}} & & \\[-4pt]
  \Big\{ & $\act{d}=l$,             &$\act{m}= m_\text{stay}$,   &$\act{p} \in P(t,l,\amem)$             &\Big\}  &  if&  $p=p_\text{travel}$\\
          \multicolumn{5}{c}{\tableexp{travel}} & & \\[-4pt]
  \Big\{ & $\act{d} \in L$,         &$\act{m} \in M(m)$,         &$\act{p}=p_\text{travel}$        &\Big\}  &  if&  $p = p_{\text{end}}$ \\
            \multicolumn{5}{c}{\tableexp{stay/end}} & & \\[-4pt]
  \Big\{ & $\act{d} = l$,           &$\act{m}=m_\text{stay}$,    &$\act{p} \in \{p,p_\text{end}\}$ &\Big\}  &  if&  $p \in P_{act}$ \text{and} $t+\Delta_p t \leq t^{\text{close}}({l,p})$ \\
            \multicolumn{5}{c}{\tableexp{end}} & & \\[-4pt]
  \Big\{ & $\act{d}=l$,             &$\act{m}=m_\text{stay}$,    &$\act{p} = p_\text{end}$         &\Big\}  &  if& $p \in P_{act}$ \text{and} $t+\Delta_p t > t^{\text{close}}({l,p})$ \\
  \end{tabular}
  }
    \right.
\end{align}
With \eqref{eq:choiceset} the choice set is defined up till: the opening and closing hours for each activity, i.e., $t^{\text{open}}({l,p})$ and $t^{\text{close}}({l,p})$; the maximum and minimum duration of each activity $\atime^{min}(p)$ and $\atime^{max}(p)$; the set of activities available depending on the history, $P_{act}(\amem)$; and the modes available depending on the mode-state, $M(m)$. 
\subsection{Conditional state transitions}
The state $\s{k+1}$ reached when taking action $a_k$ in state $\s{k}$ is a possibly random variable given by the probability density function $q(\s{k+1}|\s{k},a_k)$. To distinguish between the state variables in state $\s{k+1}$ and $\s{k}$ we will use the notation $i\ks{k}$ and $i\ks{k+1}$ for each element in the state vector.
The transitions of the state variables for location $l$, purpose $p$, time spent on activity $\atime$ and previous mode $m$ are determined directly from the state-action pair. The new location is simply the destination of the action, so $l\ks{k+1} = \act{d}$. The state variable for purpose is likewise defined as the purpose of the previous action, so $p\ks{k+1} = \act{p}$. 

The state variable for the time spent on an activity $\atime\ks{k}$ needs to store the number of times the choice has been to continue with that activity since it was last started. The counter is reset to zero whenever an activity is ending, and increases with one every time the choice is to continue for another $\Delta_p t$. An activity is continued when $\act{p} = p\ks{k}$, so the new $\atime\ks{k+1}$ is given by \eqref{eq:atimeP}.

The new mode state variable $m\ks{k+1}$ is assumed to be given by its previous value $m\ks{k}$, the mode of transport $\act{m}$ and the chosen purpose $\act{p}$ and the mapping is denoted $f_m(m\ks{k},\act{p},\act{m})$. Two examples of how the transition can be defined is discussed below in section \ref{sec:modestate}.

The stochastic of the process is gathered in the transition of the state variables for time $t$ and activity history $\amem$ and $\epsilon$, so $q(\s{k+1}|\s{k},a_k) = q(t\ks{k+1},\amem\ks{k+1},\epsilon\ks{k+1}| \s{k},a_k)$. We assume that the evolution of the need to perform activities $\xi$ may be dependent on the duration that expires between two time steps, but that the distribution of $t$ is independent on that of $\amem$. We further assume that the remaining state variables in stage $k+1$ provide sufficient statistic to determine $\epsilon_{k+1}$. These assumptions means that the joint pdf factors as. We further assume that the stochasticity in the time dimension only stems from uncertainty in travel times, and thus depends on the departure time, mode of transport, origin and destination. The distribution of $t\ks{k+1}$ is then given by \eqref{eq:tp}. The evolution of the activity history $\amem$ is assumed to be dependent only on the time of day, the previous value of $\amem$ and the purpose of the action $\act{p}$, and is thus given by \eqref{eq:amemp}. Finally, the transition probability factors as:
\begin{equation}
\begin{aligned}
	 q(t\ks{k+1},\amem\ks{k+1},\epsilon\ks{k+1}| \s{k},a_k) = & q_\epsilon(\epsilon\ks{k+1}|x\ks{k+1}) \\ &\cdot q_\amem(\cdot | t\ks{k+1},t\ks{k}, \amem\ks{k},\act{p})\\&\cdot q_t (\cdot | t\ks{k},l\ks{k},\act{m},\act{d})
\end{aligned}
\end{equation}
and the state transitions are given by:
\begin{subequations}
\begin{align}
t\ks{k+1} & \sim q_t (\cdot | t\ks{k},l\ks{k},\act{m},\act{d}) \label{eq:tp}\\
l\ks{k+1} &= \act{d} \\
p\ks{k+1} &= \act{p} \\
\atime\ks{k+1} & = \begin{cases}
                \min(\atime\ks{k} + 1,N_{\atime}) & \cif \act{p} = p\ks{k} \\
                0 & \celse 
    \end{cases} \label{eq:atimeP} \\
    m\ks{k+1} &= f_m(m\ks{k},\act{p},\act{m}) \\
\amem\ks{k+1} & \sim q_\amem(\cdot | t\ks{k+1}, t\ks{k},\amem\ks{k},\act{p}) \label{eq:amemp} \\
\epsilon\ks{k+1} & \sim q_\epsilon(\epsilon\ks{k+1}|x\ks{k+1})
\end{align}
\end{subequations}


%\subsubsection{Example: mode state variable}
%\label{sec:modestate}
%\paragraph{Last mode of transport}
%If the mode state variable $m$ denotes the last mode of transport, then:
%\begin{equation}
%    m\ks{k+1} = 
%    \begin{cases}
%        \bar{m} & \cif \bar{m} \neq m_\text{stay} \\
%        m_\text{stay} & \cif \act{p} = p_{\text{home}} \\
%        m\ks{k} & \celse
%    \end{cases}
%\end{equation}
%The benefit of this definition of $m$ is that the state variable $m\ks{k+1}$ of the destination state when performing a trip with mode $\act{m}$ is independent on the state at the origin $m\ks{k}$. If the utility function for different initial states is independent on the 
%
%\paragraph{First mode on tour away from home}
%Some models, such as \citet{Bowman01}, defines a main mode of transport for an entire tour, i.e., sequence of trips that starts and ends at home. A possible way to define such a main mode would be by the first mode used on the tour, in which case:
%\begin{equation}
%    m\ks{k+1} = 
%    \begin{cases}
%        \bar{m} & \cif \bar{m} \neq m_\text{stay} \text{ and } m=m_\text{stay} \\
%        m_\text{stay} & \cif \act{p} = p_{\text{home}} \\
%        m\ks{k} & \celse.
%    \end{cases}
%\end{equation}
%
%\subsubsection{Example: Random travel time}
%
%\subsubsection{Example: Activity memory definition and transitions}


\subsection{One-stage utility functions}
The one-stage utility $u(a_k,x_k)$ of taking an action $a_k$ in the state $s_k$ can be decomposed into the is (dis)utility of traveling $u_{trav}(a_k,s_k)$ and the utility of derived from participating or starting activity $u_{act}(a_k,s_k)$. The utility is further assumed to be additatively separable as:
\begin{equation}
\begin{aligned}
	\avgu(s_k,a_k)& =\avgu(x_k,a_k) &+ \mu_{x_k}\cdot\epsilon(a_k)\\
\end{aligned}
\end{equation}
for some constant $\mu_{x_k}$. We will assume that the expected utility of traveling is dependent on the origin $l$, destination $d$, departure time $t$, mode of transport $\act{m}$ and the mode state variable $m$. We further assume that the utility of performing an activity is dependent on the location at which it is performed $l$, the time of day $t$, the type of activity $p$, the number of time steps it has been performed $\atime$ and the activity memory state $\amem$. The one-stage utility can then be written as:
\begin{equation} \label{eq:mdponestage}
	\avgu(s_k,a_k) = \epsilon(a_k) + \begin{cases}
	\avgu_{trav}(t,l,m,\act{d},\act{m}) &\cif \act{p} = p_\text{travel} \\
	\avgu_{act}(l,t,p,\atime,\amem) & \cif \act{p} \in P_{\text{act}} \\
	0 & \cif \act{p} = p_\text{end}
	\end{cases}
\end{equation}

%\subsubsection{Example: Utility of activity participation}
%\paragraph{Time of day dependent utility}
%\paragraph{Log of duration}
%\paragraph{S-shaped}
%
%\subsubsection{Example: Utility for preferred arrival time}



